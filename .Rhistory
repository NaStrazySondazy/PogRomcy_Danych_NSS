ret
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="bytes")
ret
lapply(ret, function(x){ x[ x == na.string] <- NA; x})
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="UTF-8")
ret
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="UTF-8")
ret
htmlTable <- gsub("^.*?(<table.*</table).*$", "\\1>", doc)
htmlTable
Encoding(htmlTable)<-"UTF-8"
htmlTable
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="UTF-8")
ret
url<-paste("http://allegro.pl/listing.php/search?category=15821&sg=0&p=",1:5,"&string=facebook",sep="")
url
doc = htmlParse(url[1], encoding = "UTF-8")
doc
doc <- getURL(gdoc, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
doc
doc <- htmlParse(gdoc, encoding = "UTF-8")
doc <- getURL(gdoc, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
doc
class(doc)
gdoc <- "https://docs.google.com/spreadsheets/d/1iSt2ZD9F8DhEh8UonnYqZ71wG7gqm5MSlpiZM2vL-Gw/pubhtml?gid=1189066294&single=true"
elem <- readGoogleSheet(gdoc)
m <- cleanGoogleTable(elem, table=1)
m<-m[,colnames(m)!="X"]
m
doc <- getURL(url, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
url
doc <- getURL(gdoc, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
htmlTable <- gsub("^.*?(<table.*</table).*$", "\\1>", doc)
htmlTable
Encoding(htmlTable)
Encoding(htmlTable)<-"latin1"
Encoding(htmlTable)
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="latin1")
ret
# Przygotowanie funkcji czytającej dane
cleanGoogleTable <- function(dat, table=1, skip=0, ncols=NA, nrows=-1, header=TRUE, dropFirstCol=NA){
if(!is.data.frame(dat)){
dat <- dat[[table]]
}
if(is.na(dropFirstCol)) {
firstCol <- na.omit(dat[[1]])
if(all(firstCol == ".") || all(firstCol== as.character(seq_along(firstCol)))) {
dat <- dat[, -1]
}
} else if(dropFirstCol) {
dat <- dat[, -1]
}
if(skip > 0){
dat <- dat[-seq_len(skip), ]
}
if(nrow(dat) == 1) return(dat)
if(nrow(dat) >= 2){
if(all(is.na(dat[2, ]))) dat <- dat[-2, ]
}
if(header && nrow(dat) > 1){
header <- as.character(dat[1, ])
names(dat) <- header
dat <- dat[-1, ]
}
# Keep only desired columns
if(!is.na(ncols)){
ncols <- min(ncols, ncol(dat))
dat <- dat[, seq_len(ncols)]
}
# Keep only desired rows
if(nrows > 0){
nrows <- min(nrows, nrow(dat))
dat <- dat[seq_len(nrows), ]
}
# Rename rows
rownames(dat) <- seq_len(nrow(dat))
dat
}
# Przygotowanie funkcji czytającej dane
readGoogleSheet <- function(url, na.string="", header=TRUE){
stopifnot(require(XML))
# Suppress warnings because Google docs seems to have incomplete final line
suppressWarnings({
# doc <- paste(readLines(url), collapse=" ")
doc <- getURL(url, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
})
if(nchar(doc) == 0) stop("No content found")
htmlTable <- gsub("^.*?(<table.*</table).*$", "\\1>", doc)
ret <- readHTMLTable(htmlTable, header=header, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="UTF-8")
lapply(ret, function(x){ x[ x == na.string] <- NA; x})
}
gdoc <- "https://docs.google.com/spreadsheets/d/1iSt2ZD9F8DhEh8UonnYqZ71wG7gqm5MSlpiZM2vL-Gw/pubhtml?gid=1189066294&single=true"
elem <- readGoogleSheet(gdoc)
m <- cleanGoogleTable(elem, table=1)
m<-m[,colnames(m)!="X"]
Encoding(m)
m
Encoding(m$Miejsce zamieszkania)
Encoding(m[,"Miejsce zamieszkania"])
Encoding("ąąąććśśśśźźźźżżż")
Encoding(m[,"Miejsce zamieszkania"])<-"UTF-8"
m
Encoding(m[,"Miejsce zamieszkania"])<-"latin1"
m
gdoc <- "https://docs.google.com/spreadsheets/d/1iSt2ZD9F8DhEh8UonnYqZ71wG7gqm5MSlpiZM2vL-Gw/pubhtml?gid=1189066294&single=true"
elem <- readGoogleSheet(gdoc)
m <- cleanGoogleTable(elem, table=1)
m<-m[,colnames(m)!="X"]
Encoding(m[,"Miejsce zamieszkania"])<-"bytes"
m
Encoding(m[,"Miejsce zamieszkania"])<-"local"
m
gdoc <- "https://docs.google.com/spreadsheets/d/1iSt2ZD9F8DhEh8UonnYqZ71wG7gqm5MSlpiZM2vL-Gw/pubhtml?gid=1189066294&single=true"
elem <- readGoogleSheet(gdoc)
m <- cleanGoogleTable(elem, table=1)
m<-m[,colnames(m)!="X"]
m
Encoding("WIEĂ…Âš")
library( stringi)
stri_enc_detect("ajskdhajs")
stri_enc_detect(""WIEĂ…Âš"")
stri_enc_detect("WIEĂ…Âš")
iconv(x = "WIEĂ…Âš", from = "windows-1250", to = "UTF-8"  )
iconv(x = "WIEĂ…Âš", from = "GB18030", to = "UTF-8"  )
iconv(x = "WIEĂ…Âš", from = "Big5", to = "UTF-8"  )
Encoding(m)<-"UTF-8"
Encoding(m)
Encoding(m[,"Miejsce zamieszkania"])<-"UTF-8"
Encoding(m[,"Preferencje wyborcze"])<-"UTF-8"
m
kable(m)
kable( m, format = "markdown")
# sposob czytania arkuszy Google zaporzyczony ze strony:
# http://blog.revolutionanalytics.com/2014/06/reading-data-from-the-new-version-of-google-spreadsheets.html
# setwd( getwd() )
# Wgranie pakietów
options(OutDec= ",");
library(knitr);
library(XML);
library(RCurl)
# Przygotowanie funkcji czytającej dane
cleanGoogleTable <- function(dat, table=1, skip=0, ncols=NA, nrows=-1, header=TRUE, dropFirstCol=NA){
if(!is.data.frame(dat)){
dat <- dat[[table]]
}
if(is.na(dropFirstCol)) {
firstCol <- na.omit(dat[[1]])
if(all(firstCol == ".") || all(firstCol== as.character(seq_along(firstCol)))) {
dat <- dat[, -1]
}
} else if(dropFirstCol) {
dat <- dat[, -1]
}
if(skip > 0){
dat <- dat[-seq_len(skip), ]
}
if(nrow(dat) == 1) return(dat)
if(nrow(dat) >= 2){
if(all(is.na(dat[2, ]))) dat <- dat[-2, ]
}
if(header && nrow(dat) > 1){
header <- as.character(dat[1, ])
names(dat) <- header
dat <- dat[-1, ]
}
# Keep only desired columns
if(!is.na(ncols)){
ncols <- min(ncols, ncol(dat))
dat <- dat[, seq_len(ncols)]
}
# Keep only desired rows
if(nrows > 0){
nrows <- min(nrows, nrow(dat))
dat <- dat[seq_len(nrows), ]
}
# Rename rows
rownames(dat) <- seq_len(nrow(dat))
dat
}
# Przygotowanie funkcji czytającej dane
readGoogleSheet <- function(url, na.string="", header=TRUE){
stopifnot(require(XML))
# Suppress warnings because Google docs seems to have incomplete final line
suppressWarnings({
# doc <- paste(readLines(url), collapse=" ")
doc <- getURL(url, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
})
if(nchar(doc) == 0) stop("No content found")
htmlTable <- gsub("^.*?(<table.*</table).*$", "\\1>", doc)
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="UTF-8")
lapply(ret, function(x){ x[ x == na.string] <- NA; x})
}
# Wgrywanie  danych z google dysku
gdoc <- "https://docs.google.com/spreadsheets/d/1iSt2ZD9F8DhEh8UonnYqZ71wG7gqm5MSlpiZM2vL-Gw/pubhtml?gid=1189066294&single=true"
elem <- readGoogleSheet(gdoc)
m <- cleanGoogleTable(elem, table=1)
m<-m[,colnames(m)!="X"]
Encoding(m[,"Miejsce zamieszkania"])<-"UTF-8"
Encoding(m[,"Preferencje wyborcze"])<-"UTF-8"
kable(m)
# sposob czytania arkuszy Google zaporzyczony ze strony:
# http://blog.revolutionanalytics.com/2014/06/reading-data-from-the-new-version-of-google-spreadsheets.html
# setwd( getwd() )
# Wgranie pakietów
options(OutDec= ",");
library(knitr);
library(XML);
library(RCurl)
# Przygotowanie funkcji czytającej dane
cleanGoogleTable <- function(dat, table=1, skip=0, ncols=NA, nrows=-1, header=TRUE, dropFirstCol=NA){
if(!is.data.frame(dat)){
dat <- dat[[table]]
}
if(is.na(dropFirstCol)) {
firstCol <- na.omit(dat[[1]])
if(all(firstCol == ".") || all(firstCol== as.character(seq_along(firstCol)))) {
dat <- dat[, -1]
}
} else if(dropFirstCol) {
dat <- dat[, -1]
}
if(skip > 0){
dat <- dat[-seq_len(skip), ]
}
if(nrow(dat) == 1) return(dat)
if(nrow(dat) >= 2){
if(all(is.na(dat[2, ]))) dat <- dat[-2, ]
}
if(header && nrow(dat) > 1){
header <- as.character(dat[1, ])
names(dat) <- header
dat <- dat[-1, ]
}
# Keep only desired columns
if(!is.na(ncols)){
ncols <- min(ncols, ncol(dat))
dat <- dat[, seq_len(ncols)]
}
# Keep only desired rows
if(nrows > 0){
nrows <- min(nrows, nrow(dat))
dat <- dat[seq_len(nrows), ]
}
# Rename rows
rownames(dat) <- seq_len(nrow(dat))
dat
}
# Przygotowanie funkcji czytającej dane
readGoogleSheet <- function(url, na.string="", header=TRUE){
stopifnot(require(XML))
# Suppress warnings because Google docs seems to have incomplete final line
suppressWarnings({
# doc <- paste(readLines(url), collapse=" ")
doc <- getURL(url, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
})
if(nchar(doc) == 0) stop("No content found")
htmlTable <- gsub("^.*?(<table.*</table).*$", "\\1>", doc)
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=T, as.data.frame=TRUE, encoding="UTF-8")
lapply(ret, function(x){ x[ x == na.string] <- NA; x})
}
# Wgrywanie  danych z google dysku
gdoc <- "https://docs.google.com/spreadsheets/d/1iSt2ZD9F8DhEh8UonnYqZ71wG7gqm5MSlpiZM2vL-Gw/pubhtml?gid=1189066294&single=true"
elem <- readGoogleSheet(gdoc)
m <- cleanGoogleTable(elem, table=1)
m<-m[,colnames(m)!="X"]
Encoding(m[,"Miejsce zamieszkania"])<-"UTF-8"
Encoding(m[,"Preferencje wyborcze"])<-"UTF-8"
kable(m)
# sposob czytania arkuszy Google zaporzyczony ze strony:
# http://blog.revolutionanalytics.com/2014/06/reading-data-from-the-new-version-of-google-spreadsheets.html
# setwd( getwd() )
# Wgranie pakietów
options(OutDec= ",");
library(knitr);
library(XML);
library(RCurl)
# Przygotowanie funkcji czytającej dane
cleanGoogleTable <- function(dat, table=1, skip=0, ncols=NA, nrows=-1, header=TRUE, dropFirstCol=NA){
if(!is.data.frame(dat)){
dat <- dat[[table]]
}
if(is.na(dropFirstCol)) {
firstCol <- na.omit(dat[[1]])
if(all(firstCol == ".") || all(firstCol== as.character(seq_along(firstCol)))) {
dat <- dat[, -1]
}
} else if(dropFirstCol) {
dat <- dat[, -1]
}
if(skip > 0){
dat <- dat[-seq_len(skip), ]
}
if(nrow(dat) == 1) return(dat)
if(nrow(dat) >= 2){
if(all(is.na(dat[2, ]))) dat <- dat[-2, ]
}
if(header && nrow(dat) > 1){
header <- as.character(dat[1, ])
names(dat) <- header
dat <- dat[-1, ]
}
# Keep only desired columns
if(!is.na(ncols)){
ncols <- min(ncols, ncol(dat))
dat <- dat[, seq_len(ncols)]
}
# Keep only desired rows
if(nrows > 0){
nrows <- min(nrows, nrow(dat))
dat <- dat[seq_len(nrows), ]
}
# Rename rows
rownames(dat) <- seq_len(nrow(dat))
dat
}
# Przygotowanie funkcji czytającej dane
readGoogleSheet <- function(url, na.string="", header=TRUE){
stopifnot(require(XML))
# Suppress warnings because Google docs seems to have incomplete final line
suppressWarnings({
# doc <- paste(readLines(url), collapse=" ")
doc <- getURL(url, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
})
if(nchar(doc) == 0) stop("No content found")
htmlTable <- gsub("^.*?(<table.*</table).*$", "\\1>", doc)
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="UTF-8")
lapply(ret, function(x){ x[ x == na.string] <- NA; x})
}
# Wgrywanie  danych z google dysku
gdoc <- "https://docs.google.com/spreadsheets/d/1iSt2ZD9F8DhEh8UonnYqZ71wG7gqm5MSlpiZM2vL-Gw/pubhtml?gid=1189066294&single=true"
elem <- readGoogleSheet(gdoc)
m <- cleanGoogleTable(elem, table=1)
m<-m[,colnames(m)!="X"]
Encoding(m[,"Miejsce zamieszkania"])<-"UTF-8"
Encoding(m[,"Preferencje wyborcze"])<-"UTF-8"
kable(m)
# sposob czytania arkuszy Google zaporzyczony ze strony:
# http://blog.revolutionanalytics.com/2014/06/reading-data-from-the-new-version-of-google-spreadsheets.html
# setwd( getwd() )
# Wgranie pakietów
options(OutDec= ",");
library(knitr);
library(XML);
library(RCurl)
# Przygotowanie funkcji czytającej dane
cleanGoogleTable <- function(dat, table=1, skip=0, ncols=NA, nrows=-1, header=TRUE, dropFirstCol=NA){
if(!is.data.frame(dat)){
dat <- dat[[table]]
}
if(is.na(dropFirstCol)) {
firstCol <- na.omit(dat[[1]])
if(all(firstCol == ".") || all(firstCol== as.character(seq_along(firstCol)))) {
dat <- dat[, -1]
}
} else if(dropFirstCol) {
dat <- dat[, -1]
}
if(skip > 0){
dat <- dat[-seq_len(skip), ]
}
if(nrow(dat) == 1) return(dat)
if(nrow(dat) >= 2){
if(all(is.na(dat[2, ]))) dat <- dat[-2, ]
}
if(header && nrow(dat) > 1){
header <- as.character(dat[1, ])
names(dat) <- header
dat <- dat[-1, ]
}
# Keep only desired columns
if(!is.na(ncols)){
ncols <- min(ncols, ncol(dat))
dat <- dat[, seq_len(ncols)]
}
# Keep only desired rows
if(nrows > 0){
nrows <- min(nrows, nrow(dat))
dat <- dat[seq_len(nrows), ]
}
# Rename rows
rownames(dat) <- seq_len(nrow(dat))
dat
}
# Przygotowanie funkcji czytającej dane
readGoogleSheet <- function(url, na.string="", header=TRUE){
stopifnot(require(XML))
# Suppress warnings because Google docs seems to have incomplete final line
suppressWarnings({
# doc <- paste(readLines(url), collapse=" ")
doc <- getURL(url, ssl.verifypeer=0L, followlocation=1L) # moje wlasne usprawnienie :D
})
if(nchar(doc) == 0) stop("No content found")
htmlTable <- gsub("^.*?(<table.*</table).*$", "\\1>", doc)
ret <- readHTMLTable(htmlTable, header=T, stringsAsFactors=FALSE, as.data.frame=TRUE, encoding="UTF-8")
lapply(ret, function(x){ x[ x == na.string] <- NA; x})
}
# Wgrywanie  danych z google dysku
gdoc <- "https://docs.google.com/spreadsheets/d/1iSt2ZD9F8DhEh8UonnYqZ71wG7gqm5MSlpiZM2vL-Gw/pubhtml?gid=1189066294&single=true"
elem <- readGoogleSheet(gdoc)
m <- cleanGoogleTable(elem, table=1)
m<-m[,colnames(m)!="X"]
# Encoding(m[,"Miejsce zamieszkania"])<-"UTF-8"
# Encoding(m[,"Preferencje wyborcze"])<-"UTF-8"
kable(m)
c(rep("MIASTO",12), rep("WIEŚ",8))
c(rep("MIASTO",12), rep("WIEŚ",8))
1:20
m<-data.frame( 1:20,
c(rep("MIASTO",12), rep("WIEŚ",8)),
c( rep("Głosuję na partię X", 5),
"Głosuję na inną partię niż X",
rep("Nie idę na wybory", 6),
rep("Głosuję na partię X", 2),
rep("Głosuję na inną partię niż X", 2),
rep("Nie idę na wybory", 6)
)
m<-data.frame( 1:20,
c(rep("MIASTO",12), rep("WIEŚ",8)),
c( rep("Głosuję na partię X", 5),
"Głosuję na inną partię niż X",
rep("Nie idę na wybory", 6),
rep("Głosuję na partię X", 2),
rep("Głosuję na inną partię niż X", 2),
rep("Nie idę na wybory", 6)
)
)
c( rep("Głosuję na partię X", 5),
"Głosuję na inną partię niż X",
rep("Nie idę na wybory", 6),
rep("Głosuję na partię X", 2),
rep("Głosuję na inną partię niż X", 2),
rep("Nie idę na wybory", 6)
)
m<-data.frame( 1:20,
c(rep("MIASTO",12), rep("WIEŚ",8)),
c( rep("Głosuję na partię X", 5),
"Głosuję na inną partię niż X",
rep("Nie idę na wybory", 6),
rep("Głosuję na partię X", 2),
rep("Głosuję na inną partię niż X", 2),
rep("Nie idę na wybory", 4)
)
)
colnames(m)
colnames(m)<-c("L.p.", "Miejsce zamieszkania",	"Preferencje wyborcze")
m
kable( m, format = "markdown")
p2<-c();
for (i in 1:10000){
p2[i]<-paste(sum(sample(m$Glosowanie, 2)=="Nie idę na wybory")/2*100, "%", sep="")
}
p2<-data.frame( Lp=c(1:length(p2)), absencja=p2);
p2<-c();
for (i in 1:10000){
p2[i]<-paste(sum(sample(m$Glosowanie, 2)=="Nie idę na wybory")/2*100, "%", sep="")
}
p2<-c();
for (i in 1:10000){
p2[i]<-paste(sum(sample(m$"Miejsce zamieszkania", 2)=="Nie idę na wybory")/2*100, "%", sep="")
}
p2<-data.frame( Lp=c(1:length(p2)), absencja=p2);
p2
table(p2)
p2
p2<-c();
for (i in 1:10000){
p2[i]<-paste(sum(sample(m$"Miejsce zamieszkania", 2)=="Nie idę na wybory")/2*100, "%", sep="")
}
table(p2)
m$"Miejsce zamieszkania"
p2<-c();
for (i in 1:10000){
p2[i]<-paste(sum(sample(m$"Preferencje wyborcze", 2)=="Nie idę na wybory")/2*100, "%", sep="")
}
table(p2)
p2<-c();
for (i in 1:10000){
p2[i]<-paste(sum(sample(m$"Preferencje wyborcze", 2)=="Nie idę na wybory")/2*100, "%", sep="")
}
p2<-table(p2)
b.2<-barplot(p2, las=1)
p2<-c();
for (i in 1:10000){
p2[i]<-paste(sum(sample(m$"Preferencje wyborcze", 2)=="Nie idę na wybory")/2*100, "%", sep="")
}
p2<-table(p2)[c(1,3,2)]
b.2<-barplot(p2, las=1)
b.2<-barplot(p2, las=1, ylim=c(0,6000))
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey90")
abline(h=seq(0,6000,1000), col="grey100");
abline(v=b.2, col="grey100");
barplot(p.2, las=1, add=T, col="black", ylim=c(0, 230), ylab = "Liczba prób", xlab="Poziom absencji wyborczej w próbie", cex.names = 1.5 );
barplot(b.2, las=1, add=T, col="black", ylim=c(0, 230), ylab = "Liczba prób", xlab="Poziom absencji wyborczej w próbie", cex.names = 1.5 );
b.2
barplot(p2, las=1, add=T, col="black", ylim=c(0, 230), ylab = "Liczba prób", xlab="Poziom absencji wyborczej w próbie", cex.names = 1.5 );
b.2<-barplot(p2, las=1, ylim=c(0,6000), names.arg = "")
rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "grey90")
abline(h=seq(0,6000,1000), col="grey100");
abline(v=b.2, col="grey100");
barplot(p2, las=1, add=T, col="black", ylim=c(0, 230), ylab = "Liczba prób", xlab="Poziom absencji wyborczej w próbie", cex.names = 1.5 );
box( col = "grey90")
p2
text(b.6, p2+500, labels = p2, cex = 1.4);
p2
p2[1]
p2[[1]]
library(knitr)
p2<-c();
for (i in 1:10000){
p2[i]<-paste(sum(sample(m$"Preferencje wyborcze", 2)=="Nie idę na wybory")/2*100, "%", sep="")
}
p2<-table(p2)[c(1,3,2)]
